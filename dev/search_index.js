var documenterSearchIndex = {"docs":
[{"location":"design_matrix/#DesignMatrices","page":"Design Matrices","title":"Design Matrices","text":"","category":"section"},{"location":"design_matrix/#API","page":"Design Matrices","title":"API","text":"It is often convenient to generate multiple independent sequences, for error estimation (uncertainty quantification). The resulting sequences can be stored in what is often called a design matrix. In this package, this is achieved with the generate_design_matrices(n, d, ::DeterministicSamplingAlgorithm), ::RandomizationMethod, num_mats) function. num_mats is the number of independent realizations. The resulting design matrix is a vector of matrix of length num_mats.\n\nInstead of generating num_mats matrices, it is possible (and more memory efficient) to randomize the same matrix multiple times and perform an operation after each randomization. This is possible using iterators. To build an iterator use the DesignMatrix function.\n\nwarning: Warning\nThe method generate_design_matrices(n, d, sampler, R::NoRand, num_mats, T = Float64) is an ad hoc way to produce a Design Matrix. Indeed, it creates a deterministic point set in dimension d Ã— num_mats and splits it into num_mats point set of dimension d. The resulting sequences have no QMC guarantees. This seems to have been proposed in Section 5.1 of Saltelli, A. et al. (2010)[1] to do uncertainty quantification. See this discussion for a visual proof.\n\n[1]: Saltelli, A., Annoni, P., Azzini, I., Campolongo, F., Ratto, M., & Tarantola, S. (2010). Variance based sensitivity analysis of model output. Design and estimator for the total sensitivity index. Computer physics communications, 181(2), 259-270.","category":"section"},{"location":"design_matrix/#Example","page":"Design Matrices","title":"Example","text":"using QuasiMonteCarlo, Random, StatsBase\nRandom.seed!(1234)\nm = 4\nd = 3\nb = QuasiMonteCarlo.nextprime(d)\nN = b^m # Number of points\npad = 2m # Can also choose something as `2m` to get \"better\" randomization\nnum_mats = 5\n\nf(x) = prod(x) * 2^length(x) # test function âˆ«f(x)dáµˆx = 1\n\n# Randomize over num_mats = 5 independent Randomized Faure sequences\niterator = DesignMatrix(N, d, FaureSample(R = OwenScramble(base = b, pad = pad)), num_mats)\n\nÎ¼ = [mean(f(c) for c in eachcol(X)) for X in iterator]\n\nUsing std(Î¼) then gives you the estimated variance of your RQMC prediction.\n\n# Or using `generate_design_matrices`. Note that this is less memory efficient since it allocate space for 5 large big matrices.\nÎ¼ = [mean(f(c) for c in eachcol(X))\n     for X in QuasiMonteCarlo.generate_design_matrices(N,\n    d,\n    FaureSample(R = OwenScramble(base = b, pad = pad)),\n    num_mats)]","category":"section"},{"location":"design_matrix/#QuasiMonteCarlo.generate_design_matrices","page":"Design Matrices","title":"QuasiMonteCarlo.generate_design_matrices","text":"generate_design_matrices(n, d, sample_method::DeterministicSamplingAlgorithm,\n    num_mats, T = Float64)\ngenerate_design_matrices(n, d, sample_method::RandomSamplingAlgorithm,\n    num_mats, T = Float64)\ngenerate_design_matrices(n, lb, ub, sample_method,\n    num_mats = 2)\n\nCreate num_mats matrices each containing a QMC point set, where:\n\nn is the number of points to sample.\nd is the dimensionality of the point set in [0, 1)áµˆ,\nsample_method is the quasi-Monte Carlo sampling strategy used to create a deterministic point set out.\nT is the eltype of the point sets. For some QMC methods (Faure, Sobol) this can be Rational If the bound lb and ub are specified instead of d, the samples will be transformed into the box [lb, ub].\n\n\n\n\n\ngenerate_design_matrices(n, d, sampler, R::NoRand, num_mats, T = Float64)\n\nR = NoRand() produces num_mats matrices each containing a different deterministic point set in [0, 1)áµˆ. Note that this is an ad hoc way to produce i.i.d sequence as it creates a deterministic point in dimension d Ã— num_mats and split it in num_mats point set of dimension d. This does not have any QMC garantuees.\n\n\n\n\n\n","category":"function"},{"location":"design_matrix/#QuasiMonteCarlo.DesignMatrix","page":"Design Matrices","title":"QuasiMonteCarlo.DesignMatrix","text":"DesignMatrix(n, d, sample_method::DeterministicSamplingAlgorithm, num_mats, T = Float64)\n\nCreate an iterator for doing multiple i.i.d. randomization over QMC sequences where\n\nnum_mats is the length of the iterator\nn is the number of points to sample.\nd is the dimensionality of the point set in [0, 1)áµˆ,\nsample_method is the quasi-Monte Carlo sampling strategy used to create a deterministic point set out.\nT is the eltype of the point sets. For some QMC methods (Faure, Sobol) this can be Rational It is now compatible with all scrambling methods and shifting. One can also use it with Distributions.Sampleable or RandomSample.\n\n\n\n\n\n","category":"function"},{"location":"randomization/#Randomization","page":"Randomization methods","title":"Randomization methods","text":"Most of the methods presented in Sampler are deterministic, i.e. X = sample(n, d, ::DeterministicSamplingAlgorithm) will always produce the same sequence X = (X_1 dots X_n).\n\nThe main issue with deterministic Quasi Monte Carlo sampling is that it does not allow easy error estimation as opposed to plain Monte Carlo, where the variance can be estimated.\n\nA Randomized Quasi Monte Carlo method must respect the two following criteria:\n\nHave X_isim mathbbU(01^d) for each iin 1cdots n.\nPreserve the QMC properties, i.e. the randomized X still has low discrepancy.\n\nThis randomized version is unbiased and can be used to obtain a confidence interval or to do sensitivity analysis.\n\nA good reference is the book by A. Owen, especially the Chapters 15, 16 and 17.","category":"section"},{"location":"randomization/#API-for-randomization","page":"Randomization methods","title":"API for randomization","text":"abstract type RandomizationMethod end\n\nThere are two ways to obtain a randomized sequence:\n\nEither directly use QuasiMonteCarlo.sample(n, d, DeterministicSamplingAlgorithm(R = SomeRandomizationMethod())) or sample(n, lb, up, DeterministicSamplingAlgorithm(R = RandomizationMethod())).\nOr, given n points d-dimensional points, all in 01^d one can do randomize(X, SomeRandomizationMethod()) where X is a dtimes n-matrix.\n\nThe default method of DeterministicSamplingAlgorithm is NoRand\n\nTo obtain multiple independent randomizations of a sequence, i.e. Design Matrices, look at the Design Matrices section.\n\nnote: Note\nIn most other QMC packages, randomization is performed \"online\" as the points are samples. Here, randomization is performed after the deterministic sequence is generated. Both methods are useful in different contexts. The former is generally faster to produce one randomized sequence, while the latter is faster to produce independent realizations of the sequence.PRs are welcomed to add \"online\" version of the sequence! See this comment for inspiration.Another way to view the two approaches is: given a computational budget of N points, one canPut all of it into a sequence of size, N, thus having the best estimator hatmu_N. The price to pay is that this estimation is not associated with a variance estimation.\nDivide your computational budget into N = ntimes M to get M independent estimator hatmu_n. From there, one can compute the empirical variance of the estimator.","category":"section"},{"location":"randomization/#Scrambling-methods","page":"Randomization methods","title":"Scrambling methods","text":"abstract type ScrambleMethod <: RandomizationMethod end\n\nScramblingMethods(b, pad, rng) are well suited for (tmd)-nets in base b. b is the base used to scramble and pad the number of bits in b-ary decomposition, i.e. y simeq sum_k=1^textttpad y_ktextttb^k.\n\nThe pad is generally chosen as gtrsim log_b(n).\n\nwarning: Warning\nIn principle, the base b used for scrambling methods ScramblingMethods(b, pad, rng) can be an arbitrary integer. However, to preserve good Quasi Monte Carlo properties, it must match the base of the sequence to scramble. For example, (deterministic) Sobol sequences are base b=2, (tmd) sequences while (deterministic) Faure sequences are (tmd) sequences in prime base i.e. b is an arbitrary prime number.\n\nThe implemented ScramblingMethods are\n\nDigitalShift the simplest and fastest method. For a point xin 01^d it does y_k = (x_k + U_k) mod b where U_k sim mathbbU(0 cdots b-1)\n\nMatousekScramble a.k.a. Linear Matrix Scramble is what people use in practice. Indeed, the observed performances are similar to OwenScramble for a lesser numerical cost.\n\nOwenScramble a.k.a. Nested Uniform Scramble is the most understood theoretically, but is more costly to operate.","category":"section"},{"location":"randomization/#Other-methods","page":"Randomization methods","title":"Other methods","text":"Shift(rng) a.k.a. Cranley-Patterson Rotation. It is by far the fastest method; it is used in LatticeRuleScramble for example.","category":"section"},{"location":"randomization/#Example","page":"Randomization methods","title":"Example","text":"Randomization of a Faure sequence with various methods.","category":"section"},{"location":"randomization/#Generation","page":"Randomization methods","title":"Generation","text":"using QuasiMonteCarlo, Random\nRandom.seed!(1234)\nm = 4\nd = 3\nb = QuasiMonteCarlo.nextprime(d)\nN = b^m # Number of points\npad = m # Can also choose something as `2m` to get \"better\" randomization\n\n# Unrandomized low discrepancy sequence\nx_faure = QuasiMonteCarlo.sample(N, d, FaureSample())\n\n# Randomized version\nx_uniform = rand(d, N) # plain i.i.d. uniform\nx_shift = randomize(x_faure, Shift())\nx_nus = randomize(x_faure, OwenScramble(base = b, pad = pad)) # equivalent to sample(N, d, FaureSample(R = OwenScramble(base = b, pad = pad)))\nx_lms = randomize(x_faure, MatousekScramble(base = b, pad = pad))\nx_digital_shift = randomize(x_faure, DigitalShift(base = b, pad = pad))","category":"section"},{"location":"randomization/#Visualization-of-different-methods","page":"Randomization methods","title":"Visualization of different methods","text":"Plot the resulting sequences along dimensions 1 and 3.\n\nusing Plots\n# Setting I like for plotting\ndefault(fontfamily = \"Computer Modern\",\n    linewidth = 1,\n    label = nothing,\n    grid = true,\n    framestyle = :default)\n\nd1 = 1\nd2 = 3 # you can try every combination of dimension (d1, d2)\nsequences = [x_uniform, x_faure, x_shift, x_digital_shift, x_lms, x_nus]\nnames = [\n    \"Uniform\",\n    \"Faure (deterministic)\",\n    \"Shift\",\n    \"Digital Shift\",\n    \"Matousek Scramble\",\n    \"Owen Scramble\"\n]\np = [plot(thickness_scaling = 1.5, aspect_ratio = :equal) for i in sequences]\nfor (i, x) in enumerate(sequences)\n    scatter!(p[i], x[d1, :], x[d2, :], ms = 1.5, c = 1, grid = false)\n    title!(names[i])\n    xlims!(p[i], (0, 1))\n    ylims!(p[i], (0, 1))\n    yticks!(p[i], [0, 1])\n    xticks!(p[i], [0, 1])\n    hline!(p[i], range(0, 1, step = 1 / 4), c = :gray, alpha = 0.2)\n    vline!(p[i], range(0, 1, step = 1 / 4), c = :gray, alpha = 0.2)\n    hline!(p[i], range(0, 1, step = 1 / 2), c = :gray, alpha = 0.8)\n    vline!(p[i], range(0, 1, step = 1 / 2), c = :gray, alpha = 0.8)\nend\nplot(p..., size = (800, 600))","category":"section"},{"location":"randomization/#(t,m,d)-net-visualization","page":"Randomization methods","title":"(tmd)-net visualization","text":"Faure nets and their scrambled versions are digital (tmd)-net, which means they have strong equipartition properties. On the following plot, we can (visually) verify that with Nested Uniform Scrambling, it also works with Linear Matrix Scrambling and Digital Shift. You must see one point per rectangle of volume 1b^m. Points on the \"left\" border of rectangles are included, while those on the \"right\" are excluded. See Chapter 15.7 and Figure 15.10 for more details.\n\nd1 = 1\nd2 = 3 # you can try every combination of dimension (d1, d2)\nx = x_nus # Owen Scramble, you can try x_lms and x_digital_shift\np = [plot(thickness_scaling = 1.5, aspect_ratio = :equal) for i in 0:m]\nfor i in 0:m\n    j = m - i\n    xáµ¢ = range(0, 1, step = 1 / b^(i))\n    xâ±¼ = range(0, 1, step = 1 / b^(j))\n    scatter!(p[i + 1], x[d1, :], x[d2, :], ms = 1.5, c = 1, grid = false)\n    xlims!(p[i + 1], (0, 1.01))\n    ylims!(p[i + 1], (0, 1.01))\n    yticks!(p[i + 1], [0, 1])\n    xticks!(p[i + 1], [0, 1])\n    hline!(p[i + 1], xáµ¢, c = :gray, alpha = 0.2)\n    vline!(p[i + 1], xâ±¼, c = :gray, alpha = 0.2)\nend\nplot(p..., size = (800, 600))\n\nnote: Note\nTo check if a point set is a (tmd)-net, you can use the function istmsnet defined in the tests file of this package. It uses the excellent IntervalArithmetic.jl package.","category":"section"},{"location":"randomization/#QuasiMonteCarlo.randomize","page":"Randomization methods","title":"QuasiMonteCarlo.randomize","text":"randomize(x, R::Shift)\n\nCranley Patterson Rotation i.e. y = (x .+ U) mod 1 where U âˆ¼ ð•Œ([0,1]áµˆ) and x is a dÃ—n matrix\n\n\n\n\n\nrandomize(x, R::ScrambleMethod)\n\nReturn a scrambled version of x. The scramble methods implemented are\n\nDigitalShift.\nOwenScramble: Nested Uniform Scramble which was introduced in Owen (1995).\nMatousekScramble: Linear Matrix Scramble which was introduced in Matousek (1998).\n\n\n\n\n\n","category":"function"},{"location":"randomization/#QuasiMonteCarlo.NoRand","page":"Randomization methods","title":"QuasiMonteCarlo.NoRand","text":"NoRand <: RandomizationMethod\n\nNo randomization is performed on the sampled sequence.\n\n\n\n\n\n","category":"type"},{"location":"randomization/#QuasiMonteCarlo.ScrambleMethod","page":"Randomization methods","title":"QuasiMonteCarlo.ScrambleMethod","text":"ScrambleMethod <: RandomizationMethod\n\nA scramble method needs at least the scrambling base b, the number of \"bits\" to use pad (pad=32 is the default) and a seed rng (rng = Random.GLOBAL_RNG is the default). The scramble methods implementer are\n\nDigitalShift.\nOwenScramble: Nested Uniform Scramble which was introduced in Owen (1995).\nMatousekScramble: Linear Matrix Scramble which was introduced in Matousek (1998).\n\n\n\n\n\n","category":"type"},{"location":"randomization/#QuasiMonteCarlo.DigitalShift","page":"Randomization methods","title":"QuasiMonteCarlo.DigitalShift","text":"DigitalShift <: ScrambleMethod\n\nDigital shift. randomize(x, R::DigitalShift) returns a scrambled version of x.\n\nThe scramble method is Digital Shift. It scrambles each coordinate in base b as yâ‚– = (xâ‚– + Uâ‚–) mod b where Uâ‚– âˆ¼ ð•Œ({0:b-1}). U is the same for every point points but i.i.d. along every dimension.\n\n\n\n\n\n","category":"type"},{"location":"randomization/#QuasiMonteCarlo.MatousekScramble","page":"Randomization methods","title":"QuasiMonteCarlo.MatousekScramble","text":"MatousekScramble <: ScrambleMethod\n\nLinear Matrix Scramble aka Matousek's scramble.\n\nrandomize(x, R::MatousekScramble) returns a scrambled version of x. The scramble method is Linear Matrix Scramble which was introduced in Matousek (1998). pad is the number of bits used for each point. One needs pad â‰¥ log(base, n).\n\nReferences: MatouÅ¡ek, J. (1998). On thel2-discrepancy for anchored boxes. Journal of Complexity, 14(4), 527-556.\n\n\n\n\n\n","category":"type"},{"location":"randomization/#QuasiMonteCarlo.OwenScramble","page":"Randomization methods","title":"QuasiMonteCarlo.OwenScramble","text":"OwenScramble <: ScrambleMethod\n\nNested Uniform Scramble aka Owen's scramble.\n\nrandomize(x, R::OwenScramble) returns a scrambled version of x. The scramble method is Nested Uniform Scramble which was introduced in Owen (1995). pad is the number of bits used for each point. One needs pad â‰¥ log(base, n).\n\nReferences: Owen, A. B. (1995). Randomly permuted (t, m, s)-nets and (t, s)-sequences. In Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing: Proceedings of a conference at the University of Nevada, Las Vegas, Nevada, USA, June 23â€“25, 1994 (pp. 299-317). Springer New York.\n\n\n\n\n\n","category":"type"},{"location":"randomization/#QuasiMonteCarlo.Shift","page":"Randomization methods","title":"QuasiMonteCarlo.Shift","text":"Shifting(rng::AbstractRNG = Random.GLOBAL_RNG) <: RandomizationMethod\n\nCranley-Patterson rotation aka Shifting\n\nReferences: Cranley, R., & Patterson, T. N. (1976). Randomization of number theoretic methods for multiple integration. SIAM Journal on Numerical Analysis, 13(6), 904-914.\n\n\n\n\n\n","category":"type"},{"location":"#QuasiMonteCarlo.jl:-Quasi-Monte-Carlo-(QMC)-Samples-Made-Easy","page":"Home","title":"QuasiMonteCarlo.jl: Quasi-Monte Carlo (QMC) Samples Made Easy","text":"QuasiMonteCarlo.jl is a lightweight package for generating Quasi-Monte Carlo (QMC) samples using various different methods.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"To install QuasiMonteCarlo.jl, use the Julia package manager:\n\nusing Pkg\nPkg.add(\"QuasiMonteCarlo\")","category":"section"},{"location":"#Get-Started","page":"Home","title":"Get Started","text":"","category":"section"},{"location":"#Basic-API","page":"Home","title":"Basic API","text":"using QuasiMonteCarlo, Distributions\nlb = [0.1, -0.5]\nub = [1.0, 20.0]\nn = 5\nd = 2\n\ns = QuasiMonteCarlo.sample(n, lb, ub, GridSample())\ns = QuasiMonteCarlo.sample(n, lb, ub, Uniform())\ns = QuasiMonteCarlo.sample(n, lb, ub, SobolSample())\ns = QuasiMonteCarlo.sample(n, lb, ub, LatinHypercubeSample())\ns = QuasiMonteCarlo.sample(n, lb, ub, LatticeRuleSample())\ns = QuasiMonteCarlo.sample(n, lb, ub, HaltonSample())\ns = QuasiMonteCarlo.sample(n, lb, ub, RandomizedHaltonSample())\n\nThe output s is a matrix, so one can use things like @uview from UnsafeArrays.jl for a stack-allocated view of the ith point:\n\nusing UnsafeArrays\n@uview s[:, i]","category":"section"},{"location":"#MC-vs-QMC","page":"Home","title":"MC vs QMC","text":"We illustrate the gain of QMC methods over plain Monte Carlo using the 5-dimensional example from Section 15.9 in the book by A. Owen.\n\nfâ‚(ð±) = prod(1 + âˆš(12) / 5 * (xâ±¼ - 1 / 2) for xâ±¼ in ð±)\nÎ¼_exact = 1 # = âˆ« fâ‚(ð±) dâµð±\n\nOne can estimate the integral mu using plain Monte Carlo, or Quasi Monte Carlo or Randomized Quasi Monte Carlo. See the other section of this documentation for more information on the functions used in the example.\n\nusing QuasiMonteCarlo, Random, Distributions\nusing Plots;\ndefault(fontfamily = \"Computer Modern\");\nRandom.seed!(1234)\nd = 5 # Dimension (= prime base for Faure net)\nb = 2 # Base for Sobol net\nm_max = 19\nm_max_Faure = 8\nN = b^m_max\n\n# Generate sequences\nseq_MC = QuasiMonteCarlo.sample(N, d, Uniform()) # Monte Carlo i.i.d. Uniform sampling\nseq_QMC_Sobol = QuasiMonteCarlo.sample(N, d, SobolSample()) # Sobol net\nseq_RQMC_Sobol = QuasiMonteCarlo.sample(N,\n    d,\n    SobolSample(R = OwenScramble(base = b, pad = 32))) # Randomized version of Sobol net\nseq_RQMC_Faure = QuasiMonteCarlo.sample(d^m_max_Faure,\n    d,\n    FaureSample(R = OwenScramble(base = d, pad = 32))) # Randomized version of Faure net\n\n# Estimate the integral for different n with different estimator Î¼Ì‚â‚™\nÎ¼_MC = [mean(fâ‚(x) for x in eachcol(seq_MC[:, 1:(b^m)])) for m in 1:m_max]\nÎ¼_QMC_Sobol = [mean(fâ‚(x) for x in eachcol(seq_QMC_Sobol[:, 1:(b^m)])) for m in 1:m_max]\nÎ¼_RQMC_Sobol = [mean(fâ‚(x) for x in eachcol(seq_RQMC_Sobol[:, 1:(b^m)])) for m in 1:m_max]\nÎ¼_RQMC_Faure = [mean(fâ‚(x) for x in eachcol(seq_RQMC_Faure[:, 1:(d^m)]))\n                for m in 1:m_max_Faure]\n\n# Plot the error |Î¼Ì‚-Î¼| vs n\nplot(b .^ (1:m_max), abs.(Î¼_MC .- Î¼_exact), label = \"MC\")\nplot!(b .^ (1:m_max), abs.(Î¼_QMC_Sobol .- Î¼_exact), label = \"QMC Sobol\")\nplot!(b .^ (1:m_max), abs.(Î¼_RQMC_Sobol .- Î¼_exact), label = \"RQMC Sobol\")\nplot!(d .^ (1:m_max_Faure), abs.(Î¼_RQMC_Faure .- Î¼_exact), label = \"RQMC Faure\")\nplot!(n -> n^(-1 / 2), b .^ (1:m_max), c = :black, s = :dot, label = \"n^(-1/2)\")\nplot!(n -> n^(-3 / 2), b .^ (1:m_max), c = :black, s = :dash, label = \"n^(-3/2)\")\n# n^(-3/2) is the theoretical scaling for scrambled nets e.g. Theorem 17.5. in https://artowen.su.domains/mc/qmcstuff.pdf\nxlims!(1, 1e6)\nylims!(1e-9, 1)\nxaxis!(:log10)\nyaxis!(:log10)\nxlabel!(\"n\", legend = :bottomleft)\nylabel!(\"|Î¼Ì‚-Î¼|\")","category":"section"},{"location":"#Adding-a-new-sampling-method","page":"Home","title":"Adding a new sampling method","text":"Adding a new sampling method is a two-step process:\n\nAdd a new SamplingAlgorithm type.\nOverload the sample function with the new type.\n\nAll sampling methods are expected to return a matrix with dimension d by n, where d is the dimension of the sample space and n is the number of samples.\n\nExample\n\nstruct NewAmazingSamplingAlgorithm{OPTIONAL} <: SamplingAlgorithm end\n\nfunction sample(n, lb, ub, ::NewAmazingSamplingAlgorithm)\n    if lb isa Number\n        ...\n        return x\n    else\n        ...\n        return reduce(hcat, x)\n    end\nend","category":"section"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"section"},{"location":"#Reproducibility","page":"Home","title":"Reproducibility","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>\n\nusing Pkg # hide\nPkg.status() # hide\n\n</details>\n\n<details><summary>and using this machine and Julia version.</summary>\n\nusing InteractiveUtils # hide\nversioninfo() # hide\n\n</details>\n\n<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>\n\nusing Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide\n\n</details>\n\nusing TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"section"},{"location":"samplers/#Sampler-APIs","page":"Sampler APIs","title":"Sampler APIs","text":"","category":"section"},{"location":"samplers/#Sample","page":"Sampler APIs","title":"Sample","text":"","category":"section"},{"location":"samplers/#Samplers","page":"Sampler APIs","title":"Samplers","text":"Samplers are divided into two subtypes\n\nabstract type SamplingAlgorithm end\nabstract type RandomSamplingAlgorithm <: SamplingAlgorithm end\nabstract type DeterministicSamplingAlgorithm <: SamplingAlgorithm end","category":"section"},{"location":"samplers/#Deterministic-Sampling-Algorithm","page":"Sampler APIs","title":"Deterministic Sampling Algorithm","text":"All DeterministicSamplingAlgorithm have NoRand() as their default RandomizationMethod, see Randomization methods and Design Matrices section for more information on randomization.\n\nwarning: Warning\nThe QuasiMonteCarlo.jl package relies on the Sobol.jl package to sample Sobol nets. The choice, there is to NOT start the sequence at 0. This is debatable, see this issue and ref therein for more context.","category":"section"},{"location":"samplers/#Random-Sampling-Algorithm","page":"Sampler APIs","title":"Random Sampling Algorithm","text":"","category":"section"},{"location":"samplers/#QuasiMonteCarlo.sample","page":"Sampler APIs","title":"QuasiMonteCarlo.sample","text":"sample(n::Integer, d::Integer, S::SamplingAlgorithm, T = Float64)\nsample(n::Integer,\n    lb::T,\n    ub::T,\n    S::SamplingAlgorithm) where {T <: Union{Base.AbstractVecOrTuple, Number}}\n\nReturn a QMC point set where:\n\nn is the number of points to sample.\nS is the quasi-Monte Carlo sampling strategy. The point set is in a d-dimensional unit box [0, 1]^d. If the bounds are specified, the sample is transformed (translation + scaling) into a box [lb, ub] where:\nlb is the lower bound for each variable. Its length fixes the dimensionality of the sample.\nub is the upper bound. Its dimension must match length(lb).\n\nIn the first method the type of the point set is specified by T while in the second method the output type is inferred from the bound types.\n\n\n\n\n\n","category":"function"},{"location":"samplers/#QuasiMonteCarlo.GridSample","page":"Sampler APIs","title":"QuasiMonteCarlo.GridSample","text":"GridSample(R::RandomizationMethod = NoRand()) <: DeterministicSamplingAlgorithm\n\nA simple rectangular grid lattice. Samples n random samples from a grid with dx = (ub -lb)/n`\n\nIn more than 2 dimensions, grids have worse discrepancy than simple Monte Carlo. As a result, they should almost never be used for multivariate integration; their use is as a starting point for other algorithms.\n\n\n\n\n\n","category":"type"},{"location":"samplers/#QuasiMonteCarlo.SobolSample","page":"Sampler APIs","title":"QuasiMonteCarlo.SobolSample","text":"SobolSample(R::RandomizationMethod = NoRand()) <: DeterministicSamplingAlgorithm\n\nSamples taken from Sobol's base-2 sequence.\n\n\n\n\n\n","category":"type"},{"location":"samplers/#QuasiMonteCarlo.FaureSample","page":"Sampler APIs","title":"QuasiMonteCarlo.FaureSample","text":"FaureSample(R::RandomizationMethod = NoRand()) <: DeterministicSamplingAlgorithm\n\nA Faure low-discrepancy sequence.\n\nFaure-distributed samples cover all dimensions evenly, using the same set of points for all variables, up to ordering.\n\nWhen scrambled, randomized Faure sequences provide worst-case guarantees that variance will be at most exp(1) â‰ˆ 2.718 times greater than for a purely Monte Carlo integral. However, they are much less efficient than the Sobol sequence at integrating functions with low effective dimension (functions where the first few inputs dominate the evaluation).\n\nThe Faure sequence in dimension s forms a (0, s)-sequence with base b = nextprime(s).\n\nA Faure sequence must have length k * base^s with k < base < 1.\n\nReferences: Faure, H. (1982). DiscrÃ©pance de suites associÃ©es Ã  un systÃ¨me de numÃ©ration (en dimension s). Acta Arith., 41, 337-351. Owen, A. B. (1997). Monte Carlo variance of scrambled net quadrature. SIAM Journal on Numerical Analysis, 34(5), 1884-1910.\n\n\n\n\n\n","category":"type"},{"location":"samplers/#QuasiMonteCarlo.LatticeRuleSample","page":"Sampler APIs","title":"QuasiMonteCarlo.LatticeRuleSample","text":"LatticeRuleSample() <: DeterministicSamplingAlgorithm\n\nGenerate a point set using a lattice rule.\n\n\n\n\n\n","category":"type"},{"location":"samplers/#QuasiMonteCarlo.HaltonSample","page":"Sampler APIs","title":"QuasiMonteCarlo.HaltonSample","text":"HaltonSample(R::RandomizationMethod = NoRand()) <: DeterministicSamplingAlgorithm\n\nCreate a Halton sequence.\n\n\n\n\n\n","category":"type"},{"location":"samplers/#QuasiMonteCarlo.GoldenSample","page":"Sampler APIs","title":"QuasiMonteCarlo.GoldenSample","text":"GoldenSample()\n\nGenerate a quasirandom Kronecker sequence using powers of the generalized golden ratio.\n\nThe harmonious, or generalized golden, ratios are defined as the solutions to the equation: x^d = x + 1\n\nWhere d is the dimension of the sequence. The Golden sequence is then equivalent to Kronecker([x^-i for i in 1:d]).\n\nWARNING: the generalized golden sequence in more than 2 dimensions is purely experimental. It likely has poor discrepancy in high dimensions, and should not be used without verifying answers against a better-known quasirandom sequence. Try a rank-1 lattice rule instead.\n\nReferences: Roberts, M. (2018). The Unreasonable Effectiveness of Quasirandom Sequences. Extreme Learning. http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/\n\n\n\n\n\n","category":"function"},{"location":"samplers/#QuasiMonteCarlo.KroneckerSample","page":"Sampler APIs","title":"QuasiMonteCarlo.KroneckerSample","text":"KroneckerSample(generator::AbstractVector, R::RandomizationMethod = NoRand()) <: DeterministicSamplingAlgorithm\n\nA Kronecker sequence is a point set generated using a vector and equation: x[i] = i * generator .% 1\n\nWhere i runs from 1 through the sample size n. This sequence will be equidistributed (uniform in the infinite limit) so long as the components of generator are linearly independent over the field of rational numbers.\n\nIf no generator is specified, a lattice based on the generalized golden ratio is used; see GoldenSample for more information.\n\nKronecker sequences are not recommended for use in more than 3 dimensions, as theory on them is sparse. LatticeRuleSample will return rank-1 lattice rules, which behave similarly to Kronecker sequences but have better properties.\n\nReferences: Leobacher, G., & Pillichshammer, F. (2014). Introduction to quasi-Monte Carlo integration and applications. Switzerland: Springer International Publishing. https://link.springer.com/content/pdf/10.1007/978-3-319-03425-6.pdf\n\n\n\n\n\n","category":"type"},{"location":"samplers/#QuasiMonteCarlo.LatinHypercubeSample","page":"Sampler APIs","title":"QuasiMonteCarlo.LatinHypercubeSample","text":"LatinHypercubeSample(rng::AbstractRNG = Random.GLOBAL_RNG) <: RandomSamplingAlgorithm\n\nA Latin Hypercube is a point set with the property that every one-dimensional interval (i/n, i+1/n) contains exactly one point. This is a good way to sample a high-dimensional space, as it is more uniform than a random sample but does not require as many points as a full net.\n\n\n\n\n\n","category":"type"},{"location":"samplers/#QuasiMonteCarlo.RandomizedHaltonSample","page":"Sampler APIs","title":"QuasiMonteCarlo.RandomizedHaltonSample","text":"RandomizedHalton(rng::AbstractRNG = Random.GLOBAL_RNG) <: RandomSamplingAlgorithm\n\nCreate a randomized Halton sequence.\n\nReferences:\nOwen, A. (2017). *A randomized Halton algorithm in R*. https://doi.org/10.48550/arXiv.1706.02808\n\n\n\n\n\n","category":"type"},{"location":"types/#Choosing-the-right-type-(Float32,-Float64,-etc.)","page":"Choosing the right type (Float32, Float64, etc.)","title":"Choosing the right type (Float32, Float64, etc.)","text":"","category":"section"},{"location":"types/#Output-type","page":"Choosing the right type (Float32, Float64, etc.)","title":"Output type","text":"The Julia default for Floats is Float64. For QMC computations that already require a large amount of memory, this is usually not needed. In fact, like neural network packages, most QMC packages will not let you control the output type. The underlying C/C++ implementation is made with Float32 to save memory.\n\nIn this package, thanks to Julia's multiple dispatch and generic coding strategy, with a very small coding effort, one can get flexible output type, useful in different situations. For example, for research/academic purposes, it might be useful to have the exact Rational representation of a Faure or Sobol sequence, or Float16, Float64.\n\nwarning: Warning\nThis feature might not be available consistently yet for every QMC sequence or randomization method.","category":"section"},{"location":"types/#Example-different-output-types","page":"Choosing the right type (Float32, Float64, etc.)","title":"Example different output types","text":"using QuasiMonteCarlo, Random\nusing BenchmarkTools, StatsBase\nRandom.seed!(1234)\nm = 4\nd = 2\nb = QuasiMonteCarlo.nextprime(d)\nN = b^m # Number of points\npad = m # Can also choose something as `2m` to get \"better\" randomization\n\nQuasiMonteCarlo.sample(N, d, FaureSample(R = NoRand())) # Float64\n\nQuasiMonteCarlo.sample(N, d, FaureSample(R = NoRand()), Float32) # Float32\n\nQuasiMonteCarlo.sample(N, d, FaureSample(R = NoRand()), Rational) # Exact Rational","category":"section"},{"location":"types/#Intermediate-bits-array-type","page":"Choosing the right type (Float32, Float64, etc.)","title":"Intermediate bits array type","text":"For scrambling methods, large bit arrays with values in 0:base are created. Again, the default for storing is Int (i.e. generally Int64 as 64-bit Julia is the common installation) which can dramatically affect memory usage when doing multiple large computations. To avoid this, one can define the ScrambleMethod with Int32 (or else) like OwenScramble(base = Int32(b), pad = Int32(m)).[1]\n\n[1]: Currently, both pad and base must be of the same type. This might change.","category":"section"},{"location":"types/#Example","page":"Choosing the right type (Float32, Float64, etc.)","title":"Example","text":"To illustrate the memory gain, let's look at a bigger example with multiple randomization using iterators\n\nm = 6\nd = 3\nb = QuasiMonteCarlo.nextprime(d)\nN = b^m # Number of points\npad = 32 # Can also choose something as `2m` to get \"better\" randomization\nnum_mats = 50\n\nf(x) = prod(x) * 2^length(x) # test function âˆ«f(x)dáµˆx = 1\n\niterator32 = DesignMatrix(N,\n    d,\n    FaureSample(R = OwenScramble(base = Int32(b), pad = Int32(pad))),\n    num_mats)\niterator64 = DesignMatrix(N,\n    d,\n    FaureSample(R = OwenScramble(base = Int(b), pad = Int(pad))),\n    num_mats)\n@btime [mean(f(c) for c in eachcol(X)) for X in iterator32]\n# ~21.974 ms (1302 allocations: 2.18 MiB)\n@btime [mean(f(c) for c in eachcol(X)) for X in iterator64]\n# ~21.532 ms (1302 allocations: 3.01 MiB)\n\nThe memory usage drops from 3Mib to 2Mib! It is not exactly a factor 2 because of all the other computations happening. The number of allocations is the same, and the timing is similar. In larger cases, a time gain is also observed.\n\nnote: Note\nFor very large computations, one can change both the output type and the intermediate bit array types to minimize memory usage, e.g. DesignMatrix(N, d, FaureSample(R = OwenScramble(base = Int32(b), pad = Int32(pad))), num_mats, Float32)","category":"section"}]
}
